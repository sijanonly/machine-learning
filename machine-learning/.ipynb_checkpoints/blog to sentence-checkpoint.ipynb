{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/brown' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/sijan/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b9ce415ee76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mbrown_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'news'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m regexp_tagger = nltk.RegexpTagger(\n\u001b[0;32m      8\u001b[0m     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
      "\u001b[1;32m/home/sijan/workspace/dato-env/local/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sijan/workspace/dato-env/local/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/brown' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/sijan/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "# coding=UTF-8\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_train = brown.tagged_sents(categories='news')\n",
    "regexp_tagger = nltk.RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "     (r'(-|:|;)$', ':'),\n",
    "     (r'\\'*$', 'MD'),\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),\n",
    "     (r'.*able$', 'JJ'),\n",
    "     (r'^[A-Z].*$', 'NNP'),\n",
    "     (r'.*ness$', 'NN'),\n",
    "     (r'.*ly$', 'RB'),\n",
    "     (r'.*s$', 'NNS'),\n",
    "     (r'.*ing$', 'VBG'),\n",
    "     (r'.*ed$', 'VBD'),\n",
    "     (r'.*', 'NN')\n",
    "])\n",
    "unigram_tagger = nltk.UnigramTagger(brown_train, backoff=regexp_tagger)\n",
    "bigram_tagger = nltk.BigramTagger(brown_train, backoff=unigram_tagger)\n",
    "\n",
    "cfg = {}\n",
    "cfg[\"NNP+NNP\"] = \"NNP\"\n",
    "cfg[\"NN+NN\"] = \"NNI\"\n",
    "cfg[\"NNI+NN\"] = \"NNI\"\n",
    "cfg[\"JJ+JJ\"] = \"JJ\"\n",
    "cfg[\"JJ+NN\"] = \"NNI\"\n",
    "\n",
    "\n",
    "class NPExtractor(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "\n",
    "    # Split the sentence into singlw words/tokens\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        return tokens\n",
    "\n",
    "    # Normalize brown corpus' tags (\"NN\", \"NN-PL\", \"NNS\" > \"NN\")\n",
    "    def normalize_tags(self, tagged):\n",
    "        n_tagged = []\n",
    "        for t in tagged:\n",
    "            if t[1] == \"NP-TL\" or t[1] == \"NP\":\n",
    "                n_tagged.append((t[0], \"NNP\"))\n",
    "                continue\n",
    "            if t[1].endswith(\"-TL\"):\n",
    "                n_tagged.append((t[0], t[1][:-3]))\n",
    "                continue\n",
    "            if t[1].endswith(\"S\"):\n",
    "                n_tagged.append((t[0], t[1][:-1]))\n",
    "                continue\n",
    "            n_tagged.append((t[0], t[1]))\n",
    "        return n_tagged\n",
    "\n",
    "    # Extract the main topics from the sentence\n",
    "    def extract(self):\n",
    "\n",
    "        tokens = self.tokenize_sentence(self.sentence)\n",
    "        tags = self.normalize_tags(bigram_tagger.tag(tokens))\n",
    "\n",
    "        merge = True\n",
    "        while merge:\n",
    "            merge = False\n",
    "            for x in range(0, len(tags) - 1):\n",
    "                t1 = tags[x]\n",
    "                t2 = tags[x + 1]\n",
    "                key = \"%s+%s\" % (t1[1], t2[1])\n",
    "                value = cfg.get(key, '')\n",
    "                if value:\n",
    "                    merge = True\n",
    "                    tags.pop(x)\n",
    "                    tags.pop(x)\n",
    "                    match = \"%s %s\" % (t1[0], t2[0])\n",
    "                    pos = value\n",
    "                    tags.insert(x, (match, pos))\n",
    "                    break\n",
    "\n",
    "        matches = []\n",
    "        for t in tags:\n",
    "            if t[1] == \"NNP\" or t[1] == \"NNI\":\n",
    "            #if t[1] == \"NNP\" or t[1] == \"NNI\" or t[1] == \"NN\":\n",
    "                matches.append(t[0])\n",
    "        return matches\n",
    "\n",
    "\n",
    "# Main method, just run \"python np_extractor.py\"\n",
    "def main():\n",
    "\n",
    "    sentence = \"Your preparation tactics for the exam in these days play an important role in determining your scores in the exam. In fact, according to experts, at this final hour, students shouldnâ€™t adopt a preparation strategy but a revision strategy\"\n",
    "    np_extractor = NPExtractor(sentence)\n",
    "    result = np_extractor.extract()\n",
    "    print \"This sentence is about: %s\" % \", \".join(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
